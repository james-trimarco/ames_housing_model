---
title: "Ames SalePrice Model"
author: "Austin Anderson, Gregory Barber, James Trimarco"
date: "4/13/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Prep
### Load libraries
```{r}
library(tidyverse)
library(glmnet)
library(caret)
library(rstudioapi)
library(here)
```

### Read in data from csv files
```{r, warning=FALSE}
train <- read.csv(here("Kaggle_Data", "train.csv"), na.strings = c(""))

test <- read.csv(here("Kaggle_Data", "test.csv"), na.strings = c(""))
```

### Pairwise plots
Let's plot them 10 variables at a time for readability. 
```{r fig.height=10, fig.width=10}
# idx <- unlist(list(81, 1:10))
# pairs(train[idx])
# 
# idx <- unlist(list(81, 11:20))
# pairs(train[idx])
# 
# idx <- unlist(list(81, 21:30))
# pairs(train[idx])
# 
# idx <- unlist(list(81, 31:40))
# pairs(train[idx])
# 
# idx <- unlist(list(81, 41:50))
# pairs(train[idx])
# 
# idx <- unlist(list(81, 51:60))
# pairs(train[idx])
# 
# idx <- unlist(list(81, 61:70))
# pairs(train[idx])
# 
# idx <- unlist(list(81, 71:80))
# pairs(train[idx])
```


### Bind train and test data
This step ensures that all factors have the same number of levels in both training and test sets. 
```{r}
# create flag used to separate train and test sets after factorization
train$is_train <- 1; 
test$is_train <- 0
test$SalePrice <- NA

dat <- rbind(train,test) # create joined dataset
#plot(dat$X3SsnPorch, dat$SalePrice)
```

### Remove unhelpful variables
```{r}
#vars_to_remove <- c("X3SsnPorch", "Id")
vars_to_remove <- c("Id")

for (var in vars_to_remove) {
    dat[[var]] <- NULL
}
```

### Encode NAs
This is an opportunity to impute the median
```{r}
# dat$LotFrontage[dat$LotFrontage == "NA"] <- NA
# dat$GarageYrBlt[dat$GarageYrBlt == "NA"] <- NA
```


### Coerce tricky vars
These predictors are often misclassified, so we're coercing them explicitly. The NAs introduced by coercion here are correct. 
```{r}
tricky_numerics <- c("BsmtUnfSF","TotalBsmtSF", "GarageYrBlt", 
                     "LotFrontage", "MasVnrArea", "BsmtFinSF1", 
                     "BsmtFinSF2", "GarageArea")

for (var in tricky_numerics) {
    dat[[var]] <- as.integer(dat[[var]])
}

#dat$KitchenAbvGr
tricky_factors <- c("BsmtFullBath", "BsmtHalfBath", "GarageCars", "MSSubClass", "OverallCond", 
                    "FullBath", "HalfBath", "BedroomAbvGr", "KitchenAbvGr", "Fireplaces", 
                    "MoSold", "YrSold")

for (var in tricky_factors) {
    dat[[var]] <- as.factor(dat[[var]])
}
```

### Inspect a few histograms
Note: The data is not normally distributed along some predictors. 
```{r}
ggplot(dat, aes(x = GarageYrBlt)) +
    geom_histogram() +
    labs(title = "GarageYrBlt")

ggplot(dat, aes(x = LotArea)) +
    geom_histogram() +
    labs(title = "LotArea")
```

### Add polynomial vars
```{r polynomial}
LotArea_SQ <- dat$LotArea**2
YearBuilt_SQ <- dat$YearBuilt**2
YearRemodAdd_SQ <- dat$YearRemodAdd**2
BsmtFinSF1_SQ <- dat$BsmtFinSF1**2
BsmtFinSF2_SQ <- dat$BsmtFinSF2**2
X2ndFlrSF_SQ <- dat$X2ndFlrSF**2
GrLivArea_SQ <- dat$GrLivArea**2
LotFrontage_SQ <- dat$LotFrontage**2
    
dat <- cbind(dat, LotArea_SQ, YearBuilt_SQ, YearRemodAdd_SQ, 
             BsmtFinSF1_SQ, BsmtFinSF2_SQ, X2ndFlrSF_SQ, GrLivArea_SQ,
             LotFrontage_SQ)

head(dat)
```

### Refactor factors
Here's where we enforce consistent levels across the factors in the test and train sets. 
```{r}
fact_idx <- sapply(dat, function(x) is.factor(x))   
fact_vars <- names(dat[fact_idx])

quant_idx <- sapply(dat, function(x) is.numeric(x))
quant_idx[c("is_train", "SalePrice")] <- FALSE # take out ids, flags, response
quant_vars <- names(dat[quant_idx])
```

```{r}
quant_vars
#fact_vars
```

### Scatterplots of all quants
```{r warning=FALSE}
# plots <- list()
# idx <- 0
# for (var in quant_vars){
#   idx <- idx + 1
# 
#   p <- ggplot(dat, aes_string(x = var, y = "SalePrice")) +
#     theme_minimal() +
#     scale_y_continuous(labels = scales::dollar) +
#     geom_point(alpha = .15)
# 
#   plots[idx] <- list(p)
# }
# 
# plots

summary(dat$LotArea)

ggplot(dat, aes(x = LotArea)) +
    geom_density()

ggplot(dat, aes(x = LotArea, y = SalePrice)) +
    geom_point(alpha = .1) +
    geom_vline(aes(xintercept = quantile(train$LotArea)[4] + IQR(train$LotArea)*2.5)) +
    geom_vline(aes(xintercept = quantile(train$LotArea)[2] - IQR(train$LotArea)*2.5))

ggplot(dat, aes(x = GrLivArea, y = SalePrice)) +
  geom_point(alpha = .1) +
  geom_vline(aes(xintercept = quantile(train$GrLivArea)[4] + IQR(train$GrLivArea)*3)) +
  geom_vline(aes(xintercept = quantile(train$GrLivArea)[2] - IQR(train$GrLivArea)*3))

quantile(dat$LotArea)
# 
# dat %>%
#     group_by(PoolArea) %>%
#     summarize(median = median(SalePrice, na.rm = T))
```

```{r}
plots <- list()
idx <- 0
for (var in quant_vars){
  idx <- idx + 1

  p <- ggplot(dat, aes_string(x = var, y = "SalePrice")) +
    theme_minimal() +
    scale_y_continuous(labels = scales::dollar) +
    geom_point(alpha = .15)

  plots[idx] <- list(p)
  plots[[idx]]
}

plots[[14]]
```


### Test a few factors for length
Some are longer in the train data than in the test data. 
```{r}
# check the levels a few vars in this consolidated, train and test data sets
length(levels(dat$HouseStyle)); length(unique(test$HouseStyle))
```

### Create formula
This is used for creating a model matrix. 
```{r}
fact_formula <- as.formula(paste("~", 
                                 paste(fact_vars, sep="", collapse= "+")))

#fact_formula
```

### Separate data
Get back the train and test data
```{r}
library(rlist)
# Use the flag before to separate observations
train <- subset(dat, is_train == 1); 
test <- subset(dat, is_train == 0)
train$is_train <- NULL; test$is_train <- NULL

# remove flag from quant indices
quant_idx <- list.remove(quant_idx, range = 'is_train')

# Confirm that factor levels are now equal
length(levels(train$HouseStyle));length(levels(test$HouseStyle))
```

### Remove outliers
```{r}
outlier_vars <- c("LotArea", "BsmtFinSF1", "TotalBsmtSF", "X1stFlrSF", "GrLivArea", "GarageArea", "OpenPorchSF")

replace_outliers <- function(dataframe){
   dataframe %>%          
           map_at(outlier_vars, ~ replace(.x, .x %in% boxplot.stats(.x, coef = 3)$out, NA)) %>%
           bind_cols 
}

train <- replace_outliers(train)

train <- train %>% drop_na(outlier_vars)
```

### Define contrasts function
Required for creating model matrix
```{r}
contr.Dummy <- function(contrasts, ...){
   conT <- contr.treatment(contrasts=FALSE, ...)
   conT
}
options(contrasts=c(ordered='contr.Dummy', unordered='contr.Dummy'))
```

### Create factors
This matrix contains only the factors. The numerical data is accessed separately.
```{r}
factors_train <- model.matrix(fact_formula, data = train)
head(factors_train)[, 5]
```

## Fit on train data
```{r}
quants_train <- scale(train[, quant_idx])
#quants_train <- train[, quant_idx]

x_train  <- data.matrix(data.frame(quants_train, factors_train))
y_train <- data.matrix(train$SalePrice)

glmmod <- glmnet(x_train, y_train, alpha=1, family="gaussian")
plot(glmmod)

coef(glmmod)[, 15][coef(glmmod)[, 20] > 0]
```

### Cross validation
```{r}
cv.model <- cv.glmnet(x_train, y_train, 
                      alpha=1, type.measure = "mse", nfolds = 10)

(best.lambda <- cv.model$lambda.min)

plot(cv.model)

coef(cv.model)
```

### prep test data
```{r}
factors_test <- model.matrix(fact_formula, data = test)

quants_test <- scale(test[, quant_idx])
#quants_test <- test[, quant_idx]

x_test  <- data.matrix(data.frame(quants_test, factors_test))
```

### ::troubleshoot:: find bad rows in test data
```{r}
# x_test  <- data.matrix(data.frame(quants_test, factors_test))
# 
# na_idx <- 2121-1460
# na_idx_2 <- 2577-1460
# #x_train[1, ]
# quants_test[na_idx, ]
# factors_test[na_idx, ]
# sum(is.na(x_test[na_idx, ]))
# sum(is.na(x_test[na_idx_2, ]))
# sum(is.na(x_test[na_idx+1, ]))
# sum(is.na(x_test[na_idx-1, ]))
# sum(is.na(x_test[na_idx_2+1, ]))
# sum(is.na(x_test[na_idx_2-1, ]))
# 
# x_test[na_idx, ][1:25]
# x_test[na_idx+1, ][1:15]
```

### fix the bad rows
```{r}
na_idx <- 2121 - 1460
na_idx_2 <- 2577 - 1460

x_test[na_idx, ][is.na(x_test[na_idx, ])] <- 0
x_test[na_idx_2, ][is.na(x_test[na_idx_2, ])] <- 0
```

### make predictions
```{r}
preds_l = predict(cv.model, s=cv.model$lambda.min, newx=x_test)
sum(is.na(preds_l)) # this must be 0
head(preds_l, 5)
```

### write lasso-only csv
```{r}
preds_l <- data.frame(preds_l) %>% rownames_to_column()
colnames(preds_l) <- c("Id", "SalePrice")
head(preds_l, 5)

write.csv(preds_l, row.names = F, 
          file = "/Users/james/Documents/NCDS/Semester_2/Inference_II/ames_housing_model/Kaggle_Data/predictions_l.csv")
```

## Elastic Net
### Apply elastic net
```{r warning=FALSE}
head(data.frame(x_train))

my_control <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 5,
                           search = "random",
                           verboseIter = TRUE, 
                           allowParallel = TRUE)

my_elastic_net <- train(x_train,as.double(y_train),
                        method = "glmnet",
                        preProcess = c("center", "scale"),
                        tuneLength = 25,
                        trControl = my_control)

coef(my_elastic_net$finalModel, my_elastic_net$bestTune$lambda)
```

## Examine elastic plot
```{r}
my_elastic_net$results$RMSE
plot(my_elastic_net)
```

### Make predictions
```{r}
preds_e <- predict(my_elastic_net$finalModel, 
        s=my_elastic_net$bestTune$lambda, 
        newx=x_test)
```

### Write elasticnet csv
```{r}
preds_e <- data.frame(preds_e) %>% rownames_to_column()
colnames(preds_e) <- c("Id", "SalePrice")
head(preds_e, 5)

write.csv(preds_e, row.names = F, 
          file = "/Users/james/Documents/NCDS/Semester_2/Inference_II/ames_housing_model/Kaggle_Data/predictions_e.csv")
```