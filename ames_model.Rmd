---
title: "Untitled"
author: "Austin Anderson, Gregory Barber, James Trimarco"
date: "4/13/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Prep
### libraries
```{r}
library(tidyverse)
library(glmnet)
library(caret)
library(rstudioapi)
library(here)
```

### Read in data from csv files
```{r, warning=FALSE}
train <- read.csv(here("Kaggle_Data", "train.csv"), na.strings = c(""))

test <- read.csv(here("Kaggle_Data", "test.csv"), na.strings = c(""))
```

### Pairwise plots
Let's plot them 10 variables at a time for readability. 
```{r fig.height=10, fig.width=10}
# idx <- unlist(list(81, 1:10))
# pairs(train[idx])
# 
# idx <- unlist(list(81, 11:20))
# pairs(train[idx])
# 
# idx <- unlist(list(81, 21:30))
# pairs(train[idx])
# 
# idx <- unlist(list(81, 31:40))
# pairs(train[idx])
# 
# idx <- unlist(list(81, 41:50))
# pairs(train[idx])
# 
# idx <- unlist(list(81, 51:60))
# pairs(train[idx])
# 
# idx <- unlist(list(81, 61:70))
# pairs(train[idx])
# 
# idx <- unlist(list(81, 71:80))
# pairs(train[idx])
```

### Bind train and test data
This step ensures that all factors have the same number of levels in both training and test sets. 
```{r}
# consolidate the 2 data sets after creating a variable indicating train / test 
train$is_train <- 1; 
test$is_train <- 0
test$SalePrice <- NA
dat <- rbind(train,test)
```


### Coerce tricky vars
These predictors are often misclassified, so we're coercing them explicitly. 
```{r}
tricky_numerics <- c("BsmtUnfSF","TotalBsmtSF", "GarageYrBlt", "LotFrontage", 
                     "MasVnrArea", "BsmtFinSF1", "BsmtFinSF2", "GarageArea")

for (var in tricky_numerics) {
    dat[[var]] <- as.integer(dat[[var]])
}

tricky_factors <- c("BsmtFullBath", "BsmtHalfBath", "GarageCars")

for (var in tricky_factors) {
    dat[[var]] <- as.factor(dat[[var]])
}
```

### Inspect a few histograms
Data is not normally distributed. 
```{r}
hist(dat$GarageYrBlt)
```

### Add polynomial vars
```{r polynomial}
LotArea_SQ <- dat$LotArea**2
YearBuilt_SQ <- dat$YearBuilt**2
YearRemodAdd_SQ <- dat$YearRemodAdd**2
BsmtFinSF1_SQ <- dat$BsmtFinSF1**2
BsmtFinSF2_SQ <- dat$BsmtFinSF2**2
X2ndFlrSF_SQ <- dat$X2ndFlrSF**2
GrLivArea_SQ <- dat$GrLivArea**2
LotFrontage_SQ <- dat$LotFrontage**2
    
dat <- cbind(dat, LotArea_SQ, YearBuilt_SQ, YearRemodAdd_SQ, 
             BsmtFinSF1_SQ, BsmtFinSF2_SQ, X2ndFlrSF_SQ, GrLivArea_SQ,
             LotFrontage_SQ)
```


### Refactor factors
Here's where we enforce consistent levels across the factors in the test and train sets. 

Question: Where best to remove ID column?
```{r}
fact_idx <- sapply(dat, function(x) is.factor(x))   
fact_vars <- names(dat[fact_idx])
#fact_vars

quant_idx <- sapply(dat, function(x) is.numeric(x))
quant_idx[c("is_train", "Id", "SalePrice")] <- FALSE # take out ids, flags, response
quant_vars <- names(dat[quant_idx])
#numeric_vars
```

### Test a few factors for length
```{r}
# check the levels a few vars in this consolidated, train and test data sets
length(levels(dat$Alley)); length(unique(train$Alley));
# Here's why we need to join train and test data
length(levels(dat$HouseStyle)); length(unique(test$HouseStyle))
```

### Create formula
This is used for creating a model matrix. 
```{r}
fact_formula <- as.formula(paste("~", 
                                 paste(fact_vars, sep="", collapse= "+")))
```

### Separate data
Get back the train and test data
```{r}
# Use the flag before to separate observations
train <- subset(dat, is_train == 1); 
test <- subset(dat, is_train == 0)
train$is_train <- NULL; test$is_train <- NULL
```

### Define contrasts function
Required for creating model matrix
```{r}
contr.Dummy <- function(contrasts, ...){
   conT <- contr.treatment(contrasts=FALSE, ...)
   conT
}
options(contrasts=c(ordered='contr.Dummy', unordered='contr.Dummy'))
```

### Create factors
This matrix contains only the factors. The numerical data is accessed separately.
```{r}
factors_train <- model.matrix(fact_formula, data = train)
```

## Fit on train data
```{r}
quant_idx <- quant_idx[-(grep("is_train", names(quant_idx)))]
quants_train <- scale(train[, quant_idx])

x  <- data.matrix(data.frame(quants_train, factors_train))
y <- data.matrix(train$SalePrice)

glmmod <- glmnet(x, y, alpha=1, family="gaussian")
plot(glmmod)

coef(glmmod)[, 15][coef(glmmod)[, 20] > 0]
```

### Cross validation
```{r}
cv.model <- cv.glmnet(x, y, alpha=1, type.measure = "mse", nfolds = 10)
(best.lambda <- cv.model$lambda.min)

sqrt(cv.model$cvm[cv.model$lambda == cv.model$lambda.min])

plot(cv.model)

coef(cv.model)
```
### prep test data
```{r}
factors_test <- model.matrix(fact_formula, data = test)

quants_test <- scale(test[, quant_idx])

x  <- data.matrix(data.frame(quants_test, factors_test))
```

### make predictions
```{r}
pred = predict(cv.model, s=cv.model$lambda.min, newx=x)
head(pred)
```
## Elastic Net
## Austin messing with elastic net
```{r}
factors_train <- model.matrix(fact_formula, data = train)[, -1]

quant_idx <- quant_idx[-(grep("is_train", names(quant_idx)))]
numbers <- scale(train[, quant_idx])

x  <- data.matrix(data.frame(numbers, factors_train))
y <- data.matrix(train$SalePrice)

head(data.frame(x,y))

my_control <- trainControl(method = "repeatedcv",
                           number = 5,
                           repeats = 1,
                           search = "random",
                           verboseIter = TRUE)

my_elastic_net <- train(x,as.double(y),
                        method = "glmnet",
                        preProcess = c("center", "scale"),
                        tuneLength = 25,
                        trControl = my_control)

my_elastic_net$bestTune

coef(my_elastic_net$finalModel, my_elastic_net$bestTune$lambda)
```

```{r}
factors_test <- model.matrix(fact_formula, data = test)[, -1]

quants_test <- scale(test[, quant_idx])

x  <- data.matrix(data.frame(quants_test, factors_test))


predict(my_elastic_net$finalModel, 
        s=my_elastic_net$bestTune$lambda, 
        newx=x)
```